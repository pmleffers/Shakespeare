{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from  future import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM,Activation,Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Project Gutenberg eBook of the complete works of William Shakespeare’s dataset is used to train the network for automated text generation. Data can be downloaded from http:// www.gutenberg.org/ for the raw file used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.gutenberg.org/\n",
    "#http://www.gutenberg.org/files/100/100-0.txt\n",
    "#https://hub.packtpub.com/auto-generate-texts-shakespeare-writing-using-deep-recurrent-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to create a dictionary of characters to indices and vice-versa mapping, which we will be using to convert text into indices at later stages. This is because deep learning models cannot understand English and everything needs to be mapped into indices to train these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/pmlef/Desktop/Python_work/Selenium/Shakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path, encoding=\"utf8\").read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 5667137\n",
      "total chars: 72\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "print('corpus length:', len(text))\n",
    "print('total chars:', len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2indices = dict((c, i) for i, c in enumerate(characters))\n",
    "indices2char = dict((i, c) for i, c in enumerate(characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, various preprocessing steps are involved to make it work. The following are the major steps involved:\n",
    "\n",
    "    Preprocessing: Prepare X and Y data from the given entire story text file and converting them into indices vectorized format.\n",
    "    Deep learning model training and validation: Train and validate the deep learning model.\n",
    "    Text generation: Generate the text with the trained model.\n",
    "\n",
    "How it works…\n",
    "\n",
    "The following lines of code describe the entire modeling process of generating text from Shakespeare’s writings. Here we have chosen character length. This needs to be considered as 40 to determine the next best single character, which seems to be very fair to consider. Also, this extraction process jumps by three steps to avoid any overlapping between two consecutive extractions, to create a dataset more fairly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "np_sequence=0\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "    np_sequence=len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block is used to convert the data into a vectorized format for feeding into deep learning models, as the models cannot understand anything about text, words, sentences and so on. Initially, total dimensions are created with all zeros in the NumPy array and filled with relevant places with dictionary mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting indices into vectorized format\n",
    "\n",
    "X = np.zeros((len(sentences), maxlen, len(characters)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(characters)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char2indices[char]] = 1\n",
    "        y[i, char2indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep learning model is created with RNN, more specifically Long Short-Term Memory networks with 128 hidden neurons, and the output is in the dimensions of the characters. The number of columns in the array is the number of characters. Finally, the softmax function is used with the RMSprop optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               102912    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 72)                9288      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72)                0         \n",
      "=================================================================\n",
      "Total params: 112,200\n",
      "Trainable params: 112,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Model Building\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(characters))))\n",
    "model.add(Dense(len(characters)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, deep learning models train on number indices to map input to output (given a length of 40 characters, the model will predict the next best character). The following code is used to convert the predicted indices back to the relevant character by determining the maximum index of the character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert prediction into index\n",
    "\n",
    "def pred_indices(preds, metric=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    #if preds.all()==0: preds==10**-10\n",
    "    preds = np.log(preds+10**-10) / metric#\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds/np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be trained over 30 iterations with a batch size of 128. And also, the diversity has been changed to see the impact on the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Iteration 0\n",
      "Epoch 1/1\n",
      "1889033/1889033 [==============================] - 3099s 2ms/step - loss: 1.6567\n",
      "n----- diversity: 0.2\n",
      "----- Generating with seed: \"tor, time,\n",
      "will one day end it.\n",
      "\n",
      "ulysses\"\n",
      "n----- diversity: 0.7\n",
      "----- Generating with seed: \"tor, time,\n",
      "will one day end it.\n",
      "\n",
      "ulysses\"\n",
      "n----- diversity: 1.2\n",
      "----- Generating with seed: \"tor, time,\n",
      "will one day end it.\n",
      "\n",
      "ulysses\"\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate the Model\n",
    "\n",
    "for iteration in range(0, 1):\n",
    "    print('-' * 40)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y,batch_size=128,epochs=1)\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    gen_diversity=[]    \n",
    "    for diversity in [0.2, 0.7,1.2]:\n",
    "        print('n----- diversity:', diversity)\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        gen_diversity.append(generated)\n",
    "        \n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(characters)))\n",
    "\n",
    "        pred_sentence=[]       \n",
    "        pred_chars=[]\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char2indices[char]] = 1.\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = pred_indices(preds, diversity)\n",
    "            pred_char = indices2char[next_index]\n",
    "            generated += pred_char\n",
    "            sentence = sentence[1:] + pred_char\n",
    "            pred_sentence.append(sentence)\n",
    "            pred_chars.append(pred_char)\n",
    "            #sys.stdout.flush()\n",
    "            #print(\"nOne combination completed n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
